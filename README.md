# ðŸ§˜ AI-Based Yoga Mudra Detection System

This project detects, analyzes, and corrects **Yoga Hand Mudras** in real time using AI and computer vision.

## ðŸ§  Features
- Real-time hand tracking with MediaPipe
- Mudra classification with Deep Learning
- Feedback system for correction
- Supports multiple users and poses

## âš™ï¸ Technologies
- Python 3.10+
- OpenCV, MediaPipe, TensorFlow, NumPy, Pandas
- VS Code for development

## ðŸ“ Folder Structure
YOGA-MUDRA/
â”‚
â”œâ”€â”€ checkpoints/ â†’ Trained model weights
â”‚ â”œâ”€â”€ feedback_model.pth
â”‚ â””â”€â”€ mudra_model_best.pth
â”‚
â”œâ”€â”€ data/ â†’ Dataset and processed files
â”‚ â”œâ”€â”€ images/ â†’ Raw and captured hand mudra images
â”‚ â”œâ”€â”€ images_augmented/ â†’ Augmented (generated) images
â”‚ â”œâ”€â”€ landmarks/ â†’ Landmark data extracted via MediaPipe
â”‚ â”‚ â””â”€â”€ landmarks_clean.csv â†’ Cleaned landmark dataset
â”‚ â””â”€â”€ preprocessed/ â†’ Normalized and encoded training data
â”‚ â”œâ”€â”€ X.npy
â”‚ â”œâ”€â”€ y.npy
â”‚ â”œâ”€â”€ label_mapping.json
â”‚ â””â”€â”€ scaler.save
â”‚
â”œâ”€â”€ src/ â†’ Source code modules
â”‚ â”œâ”€â”€ capture/ â†’ Real-time webcam & prediction scripts
â”‚ â”‚ â”œâ”€â”€ capture_live.py â†’ Capture and label mudras manually
â”‚ â”‚ â”œâ”€â”€ capture_live_predict.py â†’ Live mudra prediction (model inference)
â”‚ â”‚ â””â”€â”€ capture_live_assistant.py â†’ AI-based mudra detection + feedback (audio + visual)
â”‚ â”‚
â”‚ â”œâ”€â”€ models/ â†’ Model training and architecture scripts
â”‚ â”‚ â”œâ”€â”€ train.py â†’ Train mudra classification model
â”‚ â”‚ â””â”€â”€ train_feedback_models.py â†’ Train feedback correction/assistant model
â”‚ â”‚
â”‚ â”œâ”€â”€ preprocessing/ â†’ Data preparation and cleaning scripts
â”‚ â”‚ â”œâ”€â”€ augment_and_generate_landmarks.py â†’ Data augmentation + landmark generation
â”‚ â”‚ â”œâ”€â”€ clean_landmarks.py â†’ Clean raw landmark CSVs
â”‚ â”‚ â”œâ”€â”€ generate_landmarks_csv.py â†’ Generate landmark dataset from images
â”‚ â”‚ â””â”€â”€ normalize.py â†’ Normalize + encode dataset for training
â”‚ â”‚
â”‚ â””â”€â”€ utils/ â†’ (Optional) helper scripts / utilities
â”‚
â”œâ”€â”€ venv/ â†’ Virtual environment (ignored in Git)
â”‚
â”œâ”€â”€ .gitignore â†’ Ignored files/folders list
â”œâ”€â”€ README.md â†’ Project documentation
â””â”€â”€ requirements.txt â†’ Dependencies list

### How to Run
1. Activate your virtual environment  
2. Run the capture script to start collecting data:
   \`\`\`
   python src/capture/capture_live.py user01
   \`\`\`

### Next Steps
- [ ] Collect dataset using MediaPipe
- [ ] Train baseline mudra classifier
- [ ] Add real-time correction feedback
" > README.md